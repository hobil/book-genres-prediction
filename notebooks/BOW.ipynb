{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you don't have tokenized books in the `./res` folder,\n",
    "\n",
    "download this file:\n",
    "https://drive.google.com/uc?export=download&id=1HZxpWE_T-ZhZyLAB83wO7c7dFR6_xe7T\n",
    "   \n",
    "and put its content to the `./res` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import logging\n",
    "logger = logging.getLogger().setLevel('WARNING')\n",
    "from src.utils import pickle_partial, unpickle, load_snippets\n",
    "\n",
    "import keras\n",
    "from keras import Sequential, regularizers, Model\n",
    "from keras.layers import Input, Dense, Dropout, Convolution1D, Embedding, MaxPooling1D, Flatten, Concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import collections\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "def evaluate_clf(clf_name, results, y_train, y_pred_train, y_test, y_pred_test, training_time):\n",
    "    f1_score_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    accuracy_score_train = accuracy_score(y_train, y_pred_train)\n",
    "    f1_score_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    accuracy_score_test = accuracy_score(y_test, y_pred_test)\n",
    "    results.loc[clf_name,:] = [f1_score_macro_train, f1_score_macro_test,\n",
    "                               accuracy_score_train, accuracy_score_test, training_time]\n",
    "    results.sort_values(['test f1-macro'], ascending=False, inplace=True)\n",
    "    display(results.loc[[clf_name]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents with length 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191363,)\n",
      "(33771,)\n"
     ]
    }
   ],
   "source": [
    "vocab_sizes = [1000, 2000, 5000, 10000, 20000, 30000, 40000, 50000]\n",
    "snippet_length = 200\n",
    "train_set, test_set, y_train, y_test = load_snippets(doc_size=snippet_length, word_tokenized=False)\n",
    "classes = sorted(set(y_train))\n",
    "genre_results = pd.DataFrame(columns=['train f1-macro','test f1-macro','train acc','test acc', 'train time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bigrams on train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might have to download nltk stop words\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.49 s, sys: 13.1 ms, total: 4.5 s\n",
      "Wall time: 4.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for t in train_set:\n",
    "    t.extend(['_'.join(b) for b in nltk.bigrams(t) \n",
    "           if b[0] not in stops and b[1] not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 852 ms, sys: 5.14 ms, total: 857 ms\n",
      "Wall time: 857 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for t2 in test_set:\n",
    "    t2.extend(['_'.join(b) for b in nltk.bigrams(t2) \n",
    "           if b[0] not in stops and b[1] not in stops])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary (vocabulary) if not stored yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = '../res/models/bow_dictionary_all_{}.pkl'.format(snippet_length)\n",
    "if os.path.exists(dictionary_path):\n",
    "    dictionary = gensim.corpora.Dictionary.load(dictionary_path)\n",
    "else:\n",
    "    dictionary = gensim.corpora.Dictionary(train_set)\n",
    "    dictionary.save(dictionary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>document_frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>old_man</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>young_man</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>let_us</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>could_see</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>one_day</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24525</th>\n",
       "      <td>come_back</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8394</th>\n",
       "      <td>new_york</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3476</th>\n",
       "      <td>first_time</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>long_time</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>every_one</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>last_night</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4679</th>\n",
       "      <td>little_girl</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>said_mr</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11045</th>\n",
       "      <td>years_ago</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11734</th>\n",
       "      <td>one_thing</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>came_back</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>next_day</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>go_back</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21869</th>\n",
       "      <td>said_mrs</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8972</th>\n",
       "      <td>every_day</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bigram  document_frequency\n",
       "token_id                                 \n",
       "3774          old_man                 997\n",
       "1616        young_man                 844\n",
       "1851           let_us                 760\n",
       "1622        could_see                 683\n",
       "2048          one_day                 600\n",
       "24525       come_back                 487\n",
       "8394         new_york                 476\n",
       "3476       first_time                 473\n",
       "2536        long_time                 446\n",
       "1953        every_one                 434\n",
       "938        last_night                 424\n",
       "4679      little_girl                 423\n",
       "3754          said_mr                 416\n",
       "11045       years_ago                 400\n",
       "11734       one_thing                 398\n",
       "2700        came_back                 397\n",
       "2166         next_day                 393\n",
       "621           go_back                 371\n",
       "21869        said_mrs                 353\n",
       "8972        every_day                 350"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 20\n",
    "bigram_key_val = {k:v for k,v in dictionary.token2id.items() if len(k.split('_')) > 1}\n",
    "bigram_key_val_freq = [(k,v,dictionary.dfs[v]) for k,v in bigram_key_val.items()]\n",
    "bigram_key_val_freq = sorted(bigram_key_val_freq, key=lambda x:x[2])[::-1]\n",
    "pd.DataFrame(bigram_key_val_freq, columns=['bigram', 'token_id', 'document_frequency']).set_index('token_id').head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train algorithms for given vocabulary lengths and store the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes = [1000, 2000, 5000, 10000, 20000, 30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vocab_size in vocab_sizes:\n",
    "    # process vocabulary and prepare train and test set\n",
    "    dict_filtered_path = '../res/models/bow_dictionary_{}_{}.pkl'.format(snippet_length, vocab_size)\n",
    "    if os.path.exists(dict_filtered_path):\n",
    "            dictionary = gensim.corpora.Dictionary.load(dict_filtered_path)\n",
    "    else:\n",
    "        dictionary = gensim.corpora.Dictionary.load(dictionary_path)\n",
    "        dictionary.filter_extremes(keep_n=vocab_size)\n",
    "        dictionary.save(dict_filtered_path)\n",
    "    \n",
    "    X_train = [dictionary.doc2bow(tokens) for tokens in train_set]\n",
    "    X_test = [dictionary.doc2bow(tokens) for tokens in test_set]\n",
    "\n",
    "    X_train = gensim.matutils.corpus2csc(X_train, num_terms=vocab_size).T\n",
    "    X_test = gensim.matutils.corpus2csc(X_test, num_terms=vocab_size).T\n",
    "    \n",
    "    ###################\n",
    "    ### NAIVE BAYES ###\n",
    "    ###################\n",
    "    nb = MultinomialNB()\n",
    "    t = time.time()\n",
    "    nb.fit(X_train, y_train)\n",
    "    t = time.time() - t\n",
    "    pickle.dump(nb, open('../res/models/mnb_{}_{}.pkl'.format(snippet_length,vocab_size),'wb'))\n",
    "    y_pred_train = nb.predict(X_train)\n",
    "    y_pred_test = nb.predict(X_test)\n",
    "    \n",
    "    evaluate_clf(\"MultinomialNB_{}_{}\".format(snippet_length,vocab_size),\n",
    "                 genre_results,\n",
    "                 y_train, y_pred_train, y_test, y_pred_test, t)\n",
    "    \n",
    "    nb = BernoulliNB() \n",
    "    t = time.time()\n",
    "    nb.fit(X_train, y_train)\n",
    "    t = time.time() - t\n",
    "    pickle.dump(nb, open('../res/models/bnb_{}_{}.pkl'.format(snippet_length,vocab_size),'wb'))\n",
    "    y_pred_train = nb.predict(X_train)\n",
    "    y_pred_test = nb.predict(X_test)\n",
    "    \n",
    "    evaluate_clf(\"BernoulliNB_{}_{}\".format(snippet_length,vocab_size),\n",
    "                 genre_results,\n",
    "                 y_train, y_pred_train, y_test, y_pred_test, t)\n",
    "    \n",
    "    ###########################\n",
    "    ### LOGISTIC REGRESSION ###\n",
    "    ###########################\n",
    "    clf = SGDClassifier(loss='log', max_iter=1000, n_jobs=-1, verbose=0, \n",
    "                    class_weight='balanced', random_state=42, tol=1e-6)\n",
    "    params = {\n",
    "     'alpha': [1e-3, 3e-4, 1e-4, 3e-5, 1e-5]\n",
    "    }\n",
    "    params = {\n",
    "     'alpha': [1e-3, 5e-4, 2e-4, 1e-4, 5e-5]\n",
    "    }\n",
    "    clf_log = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=params,\n",
    "        n_jobs=-1,\n",
    "        verbose=0)\n",
    "    t = time.time()\n",
    "    clf_log.fit(X_train, y_train)\n",
    "    t = time.time() - t\n",
    "    \n",
    "    pickle.dump(clf_log, open('../res/models/logreg_{}_{}.pkl'.format(snippet_length,vocab_size),'wb'))\n",
    "    y_pred_train = clf_log.predict(X_train)\n",
    "    y_pred_test = clf_log.predict(X_test)\n",
    "    evaluate_clf(\"logreg_{}_{}_{}\".format(clf_log.best_params_['alpha'], snippet_length, vocab_size),\n",
    "                 genre_results,\n",
    "                 y_train, y_pred_train, y_test, y_pred_test, t)\n",
    "    \n",
    "    ##############\n",
    "    ### TF-IDF ###\n",
    "    ##############\n",
    "    X_train = [dictionary.doc2bow(tokens) for tokens in train_set]\n",
    "    X_test = [dictionary.doc2bow(tokens) for tokens in test_set]\n",
    "    tfidf = gensim.models.TfidfModel(X_train)\n",
    "    X_train = [tfidf[x] for x in X_train]\n",
    "    X_test = [tfidf[x] for x in X_test]\n",
    "    tfidf.save('../res/models/tfidf_{}_{}.pkl'.format(snippet_length,vocab_size))\n",
    "    X_train = gensim.matutils.corpus2csc(X_train, num_terms=vocab_size).T\n",
    "    X_test = gensim.matutils.corpus2csc(X_test, num_terms=vocab_size).T\n",
    "    \n",
    "    clf = SGDClassifier(loss='log', max_iter=1000, n_jobs=-1, verbose=0, \n",
    "                    class_weight='balanced', random_state=42, tol=1e-6)\n",
    "    params = {\n",
    "     'alpha': [3e-05, 1e-05, 3e-06, 1e-06, 3e-07]\n",
    "    }\n",
    "    params = {\n",
    "     'alpha': [5e-6, 2e-6, 1e-6, 5e-7, 2e-7]\n",
    "    }\n",
    "    clf_tfidf = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=params,\n",
    "        n_jobs=-1,\n",
    "        verbose=0)\n",
    "    t = time.time()\n",
    "    clf_tfidf.fit(X_train, y_train)\n",
    "    t = time.time() - t\n",
    "    \n",
    "    pickle.dump(clf_tfidf, open('../res/models/logreg_tfidf_{}_{}.pkl'.format(snippet_length,vocab_size),'wb'))\n",
    "    y_pred_train = clf_tfidf.predict(X_train)\n",
    "    y_pred_test = clf_tfidf.predict(X_test)\n",
    "    evaluate_clf(\"tfidf_{}_{}_{}\".format(clf_tfidf.best_params_['alpha'], snippet_length, vocab_size),\n",
    "                 genre_results,\n",
    "                 y_train, y_pred_train, y_test, y_pred_test, t)\n",
    "display(genre_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train f1-macro</th>\n",
       "      <th>test f1-macro</th>\n",
       "      <th>train acc</th>\n",
       "      <th>test acc</th>\n",
       "      <th>train time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tfidf_1e-06_200_30000</th>\n",
       "      <td>0.672189</td>\n",
       "      <td>0.395043</td>\n",
       "      <td>0.655492</td>\n",
       "      <td>0.417636</td>\n",
       "      <td>45.1301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_1e-06_200_20000</th>\n",
       "      <td>0.610806</td>\n",
       "      <td>0.375022</td>\n",
       "      <td>0.597629</td>\n",
       "      <td>0.398893</td>\n",
       "      <td>46.1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_1e-05_200_30000</th>\n",
       "      <td>0.613191</td>\n",
       "      <td>0.374433</td>\n",
       "      <td>0.612877</td>\n",
       "      <td>0.398004</td>\n",
       "      <td>81.9884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB_200_30000</th>\n",
       "      <td>0.477355</td>\n",
       "      <td>0.360058</td>\n",
       "      <td>0.520341</td>\n",
       "      <td>0.413017</td>\n",
       "      <td>0.722993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB_200_20000</th>\n",
       "      <td>0.458954</td>\n",
       "      <td>0.352365</td>\n",
       "      <td>0.485324</td>\n",
       "      <td>0.395458</td>\n",
       "      <td>0.700714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB_200_30000</th>\n",
       "      <td>0.45119</td>\n",
       "      <td>0.347547</td>\n",
       "      <td>0.519824</td>\n",
       "      <td>0.411596</td>\n",
       "      <td>0.868539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_0.0001_200_20000</th>\n",
       "      <td>0.453735</td>\n",
       "      <td>0.344799</td>\n",
       "      <td>0.467133</td>\n",
       "      <td>0.377424</td>\n",
       "      <td>68.9139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB_200_20000</th>\n",
       "      <td>0.435793</td>\n",
       "      <td>0.341407</td>\n",
       "      <td>0.486259</td>\n",
       "      <td>0.394806</td>\n",
       "      <td>0.875557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB_200_10000</th>\n",
       "      <td>0.416154</td>\n",
       "      <td>0.334896</td>\n",
       "      <td>0.426916</td>\n",
       "      <td>0.366942</td>\n",
       "      <td>0.709456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_1e-05_200_10000</th>\n",
       "      <td>0.418573</td>\n",
       "      <td>0.33351</td>\n",
       "      <td>0.436605</td>\n",
       "      <td>0.363152</td>\n",
       "      <td>43.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_0.0001_200_10000</th>\n",
       "      <td>0.418569</td>\n",
       "      <td>0.328326</td>\n",
       "      <td>0.434953</td>\n",
       "      <td>0.361701</td>\n",
       "      <td>65.9516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB_200_10000</th>\n",
       "      <td>0.403481</td>\n",
       "      <td>0.322798</td>\n",
       "      <td>0.428343</td>\n",
       "      <td>0.367593</td>\n",
       "      <td>0.854213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB_200_5000</th>\n",
       "      <td>0.360785</td>\n",
       "      <td>0.306352</td>\n",
       "      <td>0.379718</td>\n",
       "      <td>0.341802</td>\n",
       "      <td>0.733815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB_200_5000</th>\n",
       "      <td>0.359501</td>\n",
       "      <td>0.304423</td>\n",
       "      <td>0.376912</td>\n",
       "      <td>0.337272</td>\n",
       "      <td>0.707446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_1e-05_200_5000</th>\n",
       "      <td>0.364036</td>\n",
       "      <td>0.301514</td>\n",
       "      <td>0.387682</td>\n",
       "      <td>0.33129</td>\n",
       "      <td>39.4294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_0.0001_200_5000</th>\n",
       "      <td>0.363219</td>\n",
       "      <td>0.298329</td>\n",
       "      <td>0.387473</td>\n",
       "      <td>0.331764</td>\n",
       "      <td>88.0216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB_200_2000</th>\n",
       "      <td>0.294803</td>\n",
       "      <td>0.266845</td>\n",
       "      <td>0.325559</td>\n",
       "      <td>0.303337</td>\n",
       "      <td>0.703568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_1e-05_200_2000</th>\n",
       "      <td>0.297685</td>\n",
       "      <td>0.263371</td>\n",
       "      <td>0.323328</td>\n",
       "      <td>0.291611</td>\n",
       "      <td>42.8947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_0.0001_200_2000</th>\n",
       "      <td>0.295114</td>\n",
       "      <td>0.263334</td>\n",
       "      <td>0.324984</td>\n",
       "      <td>0.295431</td>\n",
       "      <td>67.9322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB_200_2000</th>\n",
       "      <td>0.293266</td>\n",
       "      <td>0.262966</td>\n",
       "      <td>0.320971</td>\n",
       "      <td>0.298155</td>\n",
       "      <td>0.743439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB_200_1000</th>\n",
       "      <td>0.249374</td>\n",
       "      <td>0.233421</td>\n",
       "      <td>0.290349</td>\n",
       "      <td>0.277072</td>\n",
       "      <td>0.785137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB_200_1000</th>\n",
       "      <td>0.246842</td>\n",
       "      <td>0.23029</td>\n",
       "      <td>0.284919</td>\n",
       "      <td>0.270054</td>\n",
       "      <td>0.687697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf_1e-05_200_1000</th>\n",
       "      <td>0.253793</td>\n",
       "      <td>0.229839</td>\n",
       "      <td>0.280331</td>\n",
       "      <td>0.257854</td>\n",
       "      <td>31.0714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logreg_0.0001_200_1000</th>\n",
       "      <td>0.250684</td>\n",
       "      <td>0.228863</td>\n",
       "      <td>0.283367</td>\n",
       "      <td>0.26354</td>\n",
       "      <td>67.472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        train f1-macro test f1-macro train acc  test acc  \\\n",
       "tfidf_1e-06_200_30000         0.672189      0.395043  0.655492  0.417636   \n",
       "tfidf_1e-06_200_20000         0.610806      0.375022  0.597629  0.398893   \n",
       "logreg_1e-05_200_30000        0.613191      0.374433  0.612877  0.398004   \n",
       "MultinomialNB_200_30000       0.477355      0.360058  0.520341  0.413017   \n",
       "MultinomialNB_200_20000       0.458954      0.352365  0.485324  0.395458   \n",
       "BernoulliNB_200_30000          0.45119      0.347547  0.519824  0.411596   \n",
       "logreg_0.0001_200_20000       0.453735      0.344799  0.467133  0.377424   \n",
       "BernoulliNB_200_20000         0.435793      0.341407  0.486259  0.394806   \n",
       "MultinomialNB_200_10000       0.416154      0.334896  0.426916  0.366942   \n",
       "tfidf_1e-05_200_10000         0.418573       0.33351  0.436605  0.363152   \n",
       "logreg_0.0001_200_10000       0.418569      0.328326  0.434953  0.361701   \n",
       "BernoulliNB_200_10000         0.403481      0.322798  0.428343  0.367593   \n",
       "BernoulliNB_200_5000          0.360785      0.306352  0.379718  0.341802   \n",
       "MultinomialNB_200_5000        0.359501      0.304423  0.376912  0.337272   \n",
       "tfidf_1e-05_200_5000          0.364036      0.301514  0.387682   0.33129   \n",
       "logreg_0.0001_200_5000        0.363219      0.298329  0.387473  0.331764   \n",
       "BernoulliNB_200_2000          0.294803      0.266845  0.325559  0.303337   \n",
       "tfidf_1e-05_200_2000          0.297685      0.263371  0.323328  0.291611   \n",
       "logreg_0.0001_200_2000        0.295114      0.263334  0.324984  0.295431   \n",
       "MultinomialNB_200_2000        0.293266      0.262966  0.320971  0.298155   \n",
       "BernoulliNB_200_1000          0.249374      0.233421  0.290349  0.277072   \n",
       "MultinomialNB_200_1000        0.246842       0.23029  0.284919  0.270054   \n",
       "tfidf_1e-05_200_1000          0.253793      0.229839  0.280331  0.257854   \n",
       "logreg_0.0001_200_1000        0.250684      0.228863  0.283367   0.26354   \n",
       "\n",
       "                        train time  \n",
       "tfidf_1e-06_200_30000      45.1301  \n",
       "tfidf_1e-06_200_20000      46.1225  \n",
       "logreg_1e-05_200_30000     81.9884  \n",
       "MultinomialNB_200_30000   0.722993  \n",
       "MultinomialNB_200_20000   0.700714  \n",
       "BernoulliNB_200_30000     0.868539  \n",
       "logreg_0.0001_200_20000    68.9139  \n",
       "BernoulliNB_200_20000     0.875557  \n",
       "MultinomialNB_200_10000   0.709456  \n",
       "tfidf_1e-05_200_10000       43.056  \n",
       "logreg_0.0001_200_10000    65.9516  \n",
       "BernoulliNB_200_10000     0.854213  \n",
       "BernoulliNB_200_5000      0.733815  \n",
       "MultinomialNB_200_5000    0.707446  \n",
       "tfidf_1e-05_200_5000       39.4294  \n",
       "logreg_0.0001_200_5000     88.0216  \n",
       "BernoulliNB_200_2000      0.703568  \n",
       "tfidf_1e-05_200_2000       42.8947  \n",
       "logreg_0.0001_200_2000     67.9322  \n",
       "MultinomialNB_200_2000    0.743439  \n",
       "BernoulliNB_200_1000      0.785137  \n",
       "MultinomialNB_200_1000    0.687697  \n",
       "tfidf_1e-05_200_1000       31.0714  \n",
       "logreg_0.0001_200_1000      67.472  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(genre_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN\n",
    "with early stopping based on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_filtered_path = '../res/models/bow_dictionary_{}_{}.pkl'.format(snippet_length, vocab_size)\n",
    "if os.path.exists(dict_filtered_path):\n",
    "        dictionary = gensim.corpora.Dictionary.load(dict_filtered_path)\n",
    "else:\n",
    "    dictionary = gensim.corpora.Dictionary.load(dictionary_path)\n",
    "    dictionary.filter_extremes(keep_n=vocab_size)\n",
    "    dictionary.save(dict_filtered_path)\n",
    "\n",
    "X_train_all = [dictionary.doc2bow(tokens) for tokens in train_set]\n",
    "X_test = [dictionary.doc2bow(tokens) for tokens in test_set]\n",
    "\n",
    "X_train_all = gensim.matutils.corpus2csc(X_train_all).T\n",
    "X_test = gensim.matutils.corpus2csc(X_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191363,)\n",
      "(33771,)\n",
      "(47841, 14)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train_, y_val_ = train_test_split(X_train_all, y_train)\n",
    "\n",
    "mlb = LabelBinarizer()\n",
    "y_tr = mlb.fit_transform(list(y_train_))\n",
    "print(y_train.shape)\n",
    "y_te = mlb.transform(list(y_test))\n",
    "print(y_test.shape)\n",
    "y_val = mlb.transform(list(y_val_))\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_33 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 200)               200200    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 14)                1414      \n",
      "=================================================================\n",
      "Total params: 221,714\n",
      "Trainable params: 221,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = X_train.shape[1]\n",
    "output_shape = mlb.classes_.shape[0]\n",
    "nn = Sequential()\n",
    "nn.add(Dropout(0.4,input_shape=(n,)))\n",
    "nn.add(Dense(200, activation='relu'))\n",
    "nn.add(Dropout(0.1))\n",
    "nn.add(Dense(100, activation='relu'))\n",
    "#nn.add(Dropout(0.25))\n",
    "nn.add(Dense(output_shape, activation='softmax'))\n",
    "nn.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "display(nn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train f1-macro</th>\n",
       "      <th>test f1-macro</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nn_8</th>\n",
       "      <td>0.30232</td>\n",
       "      <td>0.232526</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.29805</td>\n",
       "      <td>77.1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_10</th>\n",
       "      <td>0.312094</td>\n",
       "      <td>0.231048</td>\n",
       "      <td>0.386171</td>\n",
       "      <td>0.297903</td>\n",
       "      <td>97.0202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_9</th>\n",
       "      <td>0.307526</td>\n",
       "      <td>0.230854</td>\n",
       "      <td>0.378123</td>\n",
       "      <td>0.29644</td>\n",
       "      <td>1.53212e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_5</th>\n",
       "      <td>0.273738</td>\n",
       "      <td>0.22685</td>\n",
       "      <td>0.352127</td>\n",
       "      <td>0.296837</td>\n",
       "      <td>1.53212e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_4</th>\n",
       "      <td>0.269353</td>\n",
       "      <td>0.226651</td>\n",
       "      <td>0.347793</td>\n",
       "      <td>0.296754</td>\n",
       "      <td>29.2598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_6</th>\n",
       "      <td>0.280819</td>\n",
       "      <td>0.226375</td>\n",
       "      <td>0.360523</td>\n",
       "      <td>0.296754</td>\n",
       "      <td>50.3621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_7</th>\n",
       "      <td>0.288041</td>\n",
       "      <td>0.224695</td>\n",
       "      <td>0.362774</td>\n",
       "      <td>0.293932</td>\n",
       "      <td>1.53212e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_3</th>\n",
       "      <td>0.257556</td>\n",
       "      <td>0.223849</td>\n",
       "      <td>0.337244</td>\n",
       "      <td>0.296357</td>\n",
       "      <td>1.53212e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_2</th>\n",
       "      <td>0.249216</td>\n",
       "      <td>0.220777</td>\n",
       "      <td>0.329127</td>\n",
       "      <td>0.293869</td>\n",
       "      <td>17.5607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_1</th>\n",
       "      <td>0.222849</td>\n",
       "      <td>0.202965</td>\n",
       "      <td>0.313666</td>\n",
       "      <td>0.287828</td>\n",
       "      <td>1.53212e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn_0</th>\n",
       "      <td>0.21031</td>\n",
       "      <td>0.196241</td>\n",
       "      <td>0.299606</td>\n",
       "      <td>0.280638</td>\n",
       "      <td>5.8529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train f1-macro test f1-macro train_accuracy test_accuracy training_time\n",
       "nn_8         0.30232      0.232526         0.3765       0.29805       77.1549\n",
       "nn_10       0.312094      0.231048       0.386171      0.297903       97.0202\n",
       "nn_9        0.307526      0.230854       0.378123       0.29644   1.53212e+09\n",
       "nn_5        0.273738       0.22685       0.352127      0.296837   1.53212e+09\n",
       "nn_4        0.269353      0.226651       0.347793      0.296754       29.2598\n",
       "nn_6        0.280819      0.226375       0.360523      0.296754       50.3621\n",
       "nn_7        0.288041      0.224695       0.362774      0.293932   1.53212e+09\n",
       "nn_3        0.257556      0.223849       0.337244      0.296357   1.53212e+09\n",
       "nn_2        0.249216      0.220777       0.329127      0.293869       17.5607\n",
       "nn_1        0.222849      0.202965       0.313666      0.287828   1.53212e+09\n",
       "nn_0         0.21031      0.196241       0.299606      0.280638        5.8529"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "143522/143522 [==============================] - 6s 45us/step - loss: 2.0762 - acc: 0.2919\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train f1-macro</th>\n",
       "      <th>test f1-macro</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nn_11</th>\n",
       "      <td>0.329576</td>\n",
       "      <td>0.230176</td>\n",
       "      <td>0.395535</td>\n",
       "      <td>0.294183</td>\n",
       "      <td>1.53212e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train f1-macro test f1-macro train_accuracy test_accuracy training_time\n",
       "nn_11       0.329576      0.230176       0.395535      0.294183   1.53212e+09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "genre_results_nn = pd.DataFrame(columns=['train f1-macro','test f1-macro','train_accuracy','test_accuracy', 'training_time'])\n",
    "clf = nn\n",
    "epochs = 0\n",
    "max_epochs = 100\n",
    "batch_size = 256\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "for i in range(max_epochs):\n",
    "    clear_output()\n",
    "    display(genre_results_nn)\n",
    "    \n",
    "    clf.fit(X_train, y_tr, batch_size=batch_size, epochs=1)\n",
    "    clf_name = 'nn_{}'.format(epochs)\n",
    "\n",
    "    t = time.time() - t\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_val = clf.predict(X_valid)\n",
    "    y_pred_train = mlb.inverse_transform(y_pred_train)\n",
    "    y_pred_val = mlb.inverse_transform(y_pred_val)\n",
    "    evaluate_clf(clf_name,\n",
    "                 genre_results_nn,\n",
    "                 y_train_, y_pred_train, y_val_, y_pred_val, t)\n",
    "\n",
    "    new_result = genre_results_nn.loc[clf_name,'test f1-macro']\n",
    "    genre_results_nn.sort_values(['test f1-macro'], ascending=False, inplace=True)\n",
    "\n",
    "    if genre_results_nn.iloc[0]['test f1-macro'] > new_result:\n",
    "        if epochs_without_improvement >= 2:\n",
    "            break\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "    else:\n",
    "        epochs_without_improvement = 0\n",
    "    \n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train f1-macro</th>\n",
       "      <th>test f1-macro</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nn_200_1000</th>\n",
       "      <td>0.329576</td>\n",
       "      <td>0.23044</td>\n",
       "      <td>0.395535</td>\n",
       "      <td>0.291167</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            train f1-macro test f1-macro train_accuracy test_accuracy  \\\n",
       "nn_200_1000       0.329576       0.23044       0.395535      0.291167   \n",
       "\n",
       "            training_time  \n",
       "nn_200_1000             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_test = mlb.inverse_transform(y_pred_test)\n",
    "evaluate_clf(f'nn_{snippet_length}_{vocab_size}',\n",
    "         genre_results,\n",
    "         y_train_, y_pred_train, y_test, y_pred_test, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_masterthesis",
   "language": "python",
   "name": "venv_masterthesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
